import os
import json
import torch
import numpy as np
import pandas as pd
from dotenv import load_dotenv
from datasets import load_dataset
from transformers import AutoTokenizer, AutoModelForTokenClassification
from peft import LoraConfig, get_peft_model
from seqeval.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report
from sklearn.metrics import confusion_matrix
import matplotlib.pyplot as plt
import seaborn as sns
from tqdm import tqdm

# –ó–∞–≥—Ä—É–∑–∫–∞ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã—Ö –æ–∫—Ä—É–∂–µ–Ω–∏—è
load_dotenv()
HF_API_TOKEN = os.getenv("HF_API_TOKEN")

class ModelTester:
    def __init__(self, model_name="Den4ikAI/rubert_large_squad_2"):
        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        print(f"üñ•Ô∏è –ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è —É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ: {self.device}")
        
        self.model_name = model_name
        self.tokenizer = AutoTokenizer.from_pretrained(model_name, token=HF_API_TOKEN)
        
        # –ó–∞–≥—Ä—É–∂–∞–µ–º –º–∞–ø–ø–∏–Ω–≥ –º–µ—Ç–æ–∫
        self.load_label_mappings()
        
    def load_label_mappings(self):
        """–ó–∞–≥—Ä—É–∂–∞–µ–º –º–∞–ø–ø–∏–Ω–≥ –º–µ—Ç–æ–∫ –∏–∑ –¥–∞—Ç–∞—Å–µ—Ç–∞"""
        # –ö–∞—Ä—Ç–∞ –º–µ—Ç–æ–∫ –¥–ª—è —Ä–∞—Å—à–∏—Ñ—Ä–æ–≤–∫–∏ (BIO-—Ñ–æ—Ä–º–∞—Ç –¥–ª—è Named Entity Recognition)
        self.label_map = {
            "0": "O",          # –í–Ω–µ —Å—É—â–Ω–æ—Å—Ç–∏ (Outside) - —Ç–æ–∫–µ–Ω –Ω–µ –æ—Ç–Ω–æ—Å–∏—Ç—Å—è –∫ –º–µ–¥–∏—Ü–∏–Ω—Å–∫–æ–π —Å—É—â–Ω–æ—Å—Ç–∏
            "1": "B-DISEASE",  # –ù–∞—á–∞–ª–æ –±–æ–ª–µ–∑–Ω–∏ (Beginning of Disease) - –ø–µ—Ä–≤—ã–π —Ç–æ–∫–µ–Ω –≤ –Ω–∞–∑–≤–∞–Ω–∏–∏ –∑–∞–±–æ–ª–µ–≤–∞–Ω–∏—è –∏–ª–∏ –¥–∏–∞–≥–Ω–æ–∑–∞
            "2": "I-DISEASE",  # –ü—Ä–æ–¥–æ–ª–∂–µ–Ω–∏–µ –±–æ–ª–µ–∑–Ω–∏ (Inside Disease) - –ø–æ—Å–ª–µ–¥—É—é—â–∏–µ —Ç–æ–∫–µ–Ω—ã –≤ –Ω–∞–∑–≤–∞–Ω–∏–∏ –∑–∞–±–æ–ª–µ–≤–∞–Ω–∏—è –∏–ª–∏ –¥–∏–∞–≥–Ω–æ–∑–∞
            "3": "B-SYMPTOM",  # –ù–∞—á–∞–ª–æ —Å–∏–º–ø—Ç–æ–º–∞ (Beginning of Symptom) - –ø–µ—Ä–≤—ã–π —Ç–æ–∫–µ–Ω –≤ –Ω–∞–∑–≤–∞–Ω–∏–∏ —Å–∏–º–ø—Ç–æ–º–∞ –∏–ª–∏ –∂–∞–ª–æ–±—ã –ø–∞—Ü–∏–µ–Ω—Ç–∞
            "4": "I-SYMPTOM",  # –ü—Ä–æ–¥–æ–ª–∂–µ–Ω–∏–µ —Å–∏–º–ø—Ç–æ–º–∞ (Inside Symptom) - –ø–æ—Å–ª–µ–¥—É—é—â–∏–µ —Ç–æ–∫–µ–Ω—ã –≤ –Ω–∞–∑–≤–∞–Ω–∏–∏ —Å–∏–º–ø—Ç–æ–º–∞ –∏–ª–∏ –∂–∞–ª–æ–±—ã –ø–∞—Ü–∏–µ–Ω—Ç–∞
            "5": "B-DRUG",     # –ù–∞—á–∞–ª–æ –ª–µ–∫–∞—Ä—Å—Ç–≤–∞ (Beginning of Drug) - –ø–µ—Ä–≤—ã–π —Ç–æ–∫–µ–Ω –≤ –Ω–∞–∑–≤–∞–Ω–∏–∏ –ª–µ–∫–∞—Ä—Å—Ç–≤–µ–Ω–Ω–æ–≥–æ –ø—Ä–µ–ø–∞—Ä–∞—Ç–∞ –∏–ª–∏ –º–µ–¥–∏–∫–∞–º–µ–Ω—Ç–∞
            "6": "I-DRUG",     # –ü—Ä–æ–¥–æ–ª–∂–µ–Ω–∏–µ –ª–µ–∫–∞—Ä—Å—Ç–≤–∞ (Inside Drug) - –ø–æ—Å–ª–µ–¥—É—é—â–∏–µ —Ç–æ–∫–µ–Ω—ã –≤ –Ω–∞–∑–≤–∞–Ω–∏–∏ –ª–µ–∫–∞—Ä—Å—Ç–≤–µ–Ω–Ω–æ–≥–æ –ø—Ä–µ–ø–∞—Ä–∞—Ç–∞ –∏–ª–∏ –º–µ–¥–∏–∫–∞–º–µ–Ω—Ç–∞
            "7": "B-ANATOMY",  # –ù–∞—á–∞–ª–æ –∞–Ω–∞—Ç–æ–º–∏–∏ (Beginning of Anatomy) - –ø–µ—Ä–≤—ã–π —Ç–æ–∫–µ–Ω –≤ –Ω–∞–∑–≤–∞–Ω–∏–∏ –æ—Ä–≥–∞–Ω–∞, —Ç–∫–∞–Ω–∏ –∏–ª–∏ —á–∞—Å—Ç–∏ —Ç–µ–ª–∞
            "8": "I-ANATOMY",  # –ü—Ä–æ–¥–æ–ª–∂–µ–Ω–∏–µ –∞–Ω–∞—Ç–æ–º–∏–∏ (Inside Anatomy) - –ø–æ—Å–ª–µ–¥—É—é—â–∏–µ —Ç–æ–∫–µ–Ω—ã –≤ –Ω–∞–∑–≤–∞–Ω–∏–∏ –æ—Ä–≥–∞–Ω–∞, —Ç–∫–∞–Ω–∏ –∏–ª–∏ —á–∞—Å—Ç–∏ —Ç–µ–ª–∞
            "9": "B-PROCEDURE", # –ù–∞—á–∞–ª–æ –ø—Ä–æ—Ü–µ–¥—É—Ä—ã (Beginning of Procedure) - –ø–µ—Ä–≤—ã–π —Ç–æ–∫–µ–Ω –≤ –Ω–∞–∑–≤–∞–Ω–∏–∏ –º–µ–¥–∏—Ü–∏–Ω—Å–∫–æ–π –ø—Ä–æ—Ü–µ–¥—É—Ä—ã –∏–ª–∏ –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏—è
            "10": "I-PROCEDURE", # –ü—Ä–æ–¥–æ–ª–∂–µ–Ω–∏–µ –ø—Ä–æ—Ü–µ–¥—É—Ä—ã (Inside Procedure) - –ø–æ—Å–ª–µ–¥—É—é—â–∏–µ —Ç–æ–∫–µ–Ω—ã –≤ –Ω–∞–∑–≤–∞–Ω–∏–∏ –º–µ–¥–∏—Ü–∏–Ω—Å–∫–æ–π –ø—Ä–æ—Ü–µ–¥—É—Ä—ã –∏–ª–∏ –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏—è
            "11": "B-FINDING", # –ù–∞—á–∞–ª–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞ (Beginning of Finding) - –ø–µ—Ä–≤—ã–π —Ç–æ–∫–µ–Ω –≤ –Ω–∞–∑–≤–∞–Ω–∏–∏ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞ –æ–±—Å–ª–µ–¥–æ–≤–∞–Ω–∏—è –∏–ª–∏ –º–µ–¥–∏—Ü–∏–Ω—Å–∫–æ–≥–æ –Ω–∞–±–ª—é–¥–µ–Ω–∏—è
            "12": "I-FINDING"  # –ü—Ä–æ–¥–æ–ª–∂–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞ (Inside Finding) - –ø–æ—Å–ª–µ–¥—É—é—â–∏–µ —Ç–æ–∫–µ–Ω—ã –≤ –Ω–∞–∑–≤–∞–Ω–∏–∏ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞ –æ–±—Å–ª–µ–¥–æ–≤–∞–Ω–∏—è –∏–ª–∏ –º–µ–¥–∏—Ü–∏–Ω—Å–∫–æ–≥–æ –Ω–∞–±–ª—é–¥–µ–Ω–∏—è
        }

        try:
            dataset = load_dataset("Rexhaif/ru-med-ner", token=HF_API_TOKEN)
            train_dataset = dataset["train"]

            if hasattr(train_dataset.features["ner_tags"], 'feature') and hasattr(train_dataset.features["ner_tags"].feature, 'names'):
                label_names = train_dataset.features["ner_tags"].feature.names
            else:
                all_labels = set()
                for example in train_dataset:
                    all_labels.update(example["ner_tags"])
                label_names = sorted(list(all_labels))

            self.id2label = {i: label for i, label in enumerate(label_names)}
            self.label2id = {label: i for i, label in enumerate(label_names)}
            self.num_labels = len(label_names)

            print(f"üìã –ó–∞–≥—Ä—É–∂–µ–Ω—ã –º–µ—Ç–∫–∏: {list(self.id2label.values())}")

        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–≥—Ä—É–∑–∫–µ –¥–∞—Ç–∞—Å–µ—Ç–∞: {e}")
            # Fallback –∫ —Å–æ—Ö—Ä–∞–Ω—ë–Ω–Ω—ã–º –º–µ—Ç–∫–∞–º
            with open("./models/lora_adapter/id2label.json", "r", encoding="utf-8") as f:
                loaded = json.load(f)
                self.id2label = {int(k): self.label_map[str(v)] for k, v in loaded.items()}
            self.label2id = {v: k for k, v in self.id2label.items()}
            self.num_labels = len(self.id2label)
    
    def load_base_model(self):
        """–ó–∞–≥—Ä—É–∂–∞–µ–º –±–∞–∑–æ–≤—É—é –º–æ–¥–µ–ª—å –±–µ–∑ LoRA"""
        print("üîÑ –ó–∞–≥—Ä—É–∑–∫–∞ –±–∞–∑–æ–≤–æ–π –º–æ–¥–µ–ª–∏...")
        base_model = AutoModelForTokenClassification.from_pretrained(
            self.model_name,
            num_labels=self.num_labels,
            id2label=self.id2label,
            label2id=self.label2id,
            token=HF_API_TOKEN,
            ignore_mismatched_sizes=True
        )
        base_model.to(self.device)
        return base_model
    
    def load_lora_model(self, adapter_path="./models/lora_adapter"):
        """–ó–∞–≥—Ä—É–∂–∞–µ–º –º–æ–¥–µ–ª—å —Å LoRA –∞–¥–∞–ø—Ç–µ—Ä–æ–º"""
        print("üîÑ –ó–∞–≥—Ä—É–∑–∫–∞ LoRA –º–æ–¥–µ–ª–∏...")
        
        # –°–Ω–∞—á–∞–ª–∞ –∑–∞–≥—Ä—É–∂–∞–µ–º –±–∞–∑–æ–≤—É—é –º–æ–¥–µ–ª—å
        base_model = self.load_base_model()
        
        # –ù–∞—Å—Ç—Ä–∞–∏–≤–∞–µ–º LoRA –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é
        lora_config = LoraConfig(
            r=8,
            lora_alpha=16,
            target_modules=["query", "value"],
            lora_dropout=0.1,
            bias="none",
            task_type="TOKEN_CLS",
        )
        
        # –ü—Ä–∏–º–µ–Ω—è–µ–º LoRA –∫ –±–∞–∑–æ–≤–æ–π –º–æ–¥–µ–ª–∏
        lora_model = get_peft_model(base_model, lora_config)
        
        # –ó–∞–≥—Ä—É–∂–∞–µ–º –≤–µ—Å–∞ –∞–¥–∞–ø—Ç–µ—Ä–∞
        try:
            lora_model.load_adapter(adapter_path, "default")
            print("‚úÖ LoRA –∞–¥–∞–ø—Ç–µ—Ä —É—Å–ø–µ—à–Ω–æ –∑–∞–≥—Ä—É–∂–µ–Ω")
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ LoRA –∞–¥–∞–ø—Ç–µ—Ä–∞: {e}")
            return None
            
        lora_model.to(self.device)
        return lora_model
    
    def predict_single_text(self, model, text):
        """–ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ –¥–ª—è –æ–¥–Ω–æ–≥–æ —Ç–µ–∫—Å—Ç–∞"""
        # –¢–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è
        inputs = self.tokenizer(
            text, 
            return_tensors="pt", 
            truncation=True, 
            padding=True,
            max_length=512,
            is_split_into_words=False
        ).to(self.device)
        
        # –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ
        with torch.no_grad():
            outputs = model(**inputs)
            predictions = torch.argmax(outputs.logits, dim=2)
        
        # –î–µ–∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏–µ —Ç–æ–∫–µ–Ω–æ–≤ –∏ –º–µ—Ç–æ–∫
        tokens = self.tokenizer.convert_ids_to_tokens(inputs["input_ids"][0])
        predicted_labels = [self.id2label[pred.item()] for pred in predictions[0]]
        # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º —á–∏—Å–ª–æ–≤—ã–µ –º–µ—Ç–∫–∏ –≤ —Å—Ç—Ä–æ–∫–æ–≤—ã–µ
        predicted_labels = [self.label_map[str(label)] for label in predicted_labels]
        
        # –§–∏–ª—å—Ç—Ä—É–µ–º —Å–ø–µ—Ü–∏–∞–ª—å–Ω—ã–µ —Ç–æ–∫–µ–Ω—ã
        filtered_results = []
        for token, label in zip(tokens, predicted_labels):
            if token not in ["[CLS]", "[SEP]", "[PAD]"]:
                filtered_results.append((token, label))
                
        return filtered_results
    
    def predict_batch(self, model, texts):
        """–ü–∞–∫–µ—Ç–Ω–æ–µ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ"""
        all_predictions = []
        
        for text in tqdm(texts, desc="–ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è"):
            pred = self.predict_single_text(model, text)
            all_predictions.append(pred)
            
        return all_predictions
    
    def evaluate_on_dataset(self, model, test_size=100):
        """–û—Ü–µ–Ω–∫–∞ –Ω–∞ —á–∞—Å—Ç–∏ –¥–∞—Ç–∞—Å–µ—Ç–∞"""
        print(f"üìä –ó–∞–≥—Ä—É–∑–∫–∞ —Ç–µ—Å—Ç–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö (—Ä–∞–∑–º–µ—Ä: {test_size})...")
        
        try:
            dataset = load_dataset("Rexhaif/ru-med-ner", token=HF_API_TOKEN)
            
            # –ë–µ—Ä—ë–º —á–∞—Å—Ç—å –¥–∞–Ω–Ω—ã—Ö –¥–ª—è —Ç–µ—Å—Ç–∞
            if "test" in dataset:
                test_data = dataset["test"].select(range(min(test_size, len(dataset["test"]))))
            else:
                # –ï—Å–ª–∏ –Ω–µ—Ç —Ç–µ—Å—Ç–æ–≤–æ–π –≤—ã–±–æ—Ä–∫–∏, –±–µ—Ä—ë–º –∏–∑ –≤–∞–ª–∏–¥–∞—Ü–∏–æ–Ω–Ω–æ–π –∏–ª–∏ –æ–±—É—á–∞—é—â–µ–π
                if "validation" in dataset:
                    test_data = dataset["validation"].select(range(min(test_size, len(dataset["validation"]))))
                else:
                    test_data = dataset["train"].select(range(-test_size, len(dataset["train"])))  # –ü–æ—Å–ª–µ–¥–Ω–∏–µ –ø—Ä–∏–º–µ—Ä—ã
            
            true_predictions = []
            true_labels = []
            
            print("üîÑ –û–±—Ä–∞–±–æ—Ç–∫–∞ –ø—Ä–∏–º–µ—Ä–æ–≤...")
            for example in tqdm(test_data):
                tokens = example["tokens"]
                true_tags = example["ner_tags"]
                
                # –°–æ–∑–¥–∞—ë–º —Ç–µ–∫—Å—Ç –∏–∑ —Ç–æ–∫–µ–Ω–æ–≤
                text = " ".join(tokens)
                
                # –ü–æ–ª—É—á–∞–µ–º –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è
                predictions = self.predict_single_text(model, text)
                
                # –ò–∑–≤–ª–µ–∫–∞–µ–º —Ç–æ–ª—å–∫–æ –º–µ—Ç–∫–∏ (–±–µ–∑ —Ç–æ–∫–µ–Ω–æ–≤)
                pred_labels = [label for _, label in predictions]
                true_label_names = [self.id2label[tag] for tag in true_tags]
                # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º —á–∏—Å–ª–æ–≤—ã–µ –º–µ—Ç–∫–∏ –≤ —Å—Ç—Ä–æ–∫–æ–≤—ã–µ
                true_label_names = [self.label_map[str(label)] for label in true_label_names]

                # –í—ã—Ä–∞–≤–Ω–∏–≤–∞–µ–º –¥–ª–∏–Ω—ã (–º–æ–∂–µ—Ç –æ—Ç–ª–∏—á–∞—Ç—å—Å—è –∏–∑-–∑–∞ –ø–æ–¥—Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏–∏)
                min_len = min(len(pred_labels), len(true_label_names))
                if min_len > 0:
                    true_predictions.append(pred_labels[:min_len])
                    true_labels.append(true_label_names[:min_len])
            
            # –í—ã—á–∏—Å–ª—è–µ–º –º–µ—Ç—Ä–∏–∫–∏
            metrics = {
                "accuracy": accuracy_score(true_labels, true_predictions),
                "precision": precision_score(true_labels, true_predictions),
                "recall": recall_score(true_labels, true_predictions),
                "f1": f1_score(true_labels, true_predictions),
            }
            
            return metrics, true_labels, true_predictions
            
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –æ—Ü–µ–Ω–∫–µ –Ω–∞ –¥–∞—Ç–∞—Å–µ—Ç–µ: {e}")
            return None, None, None
    
    def create_test_examples(self):
        """–°–æ–∑–¥–∞—ë–º —Å–æ–±—Å—Ç–≤–µ–Ω–Ω—ã–µ —Ç–µ—Å—Ç–æ–≤—ã–µ –ø—Ä–∏–º–µ—Ä—ã —Å —Ä–∞–∑–º–µ—Ç–∫–æ–π"""
        test_examples = [
            {
                "text": "–ü–∞—Ü–∏–µ–Ω—Ç –∂–∞–ª—É–µ—Ç—Å—è –Ω–∞ –±–æ–ª—å –≤ –≥—Ä—É–¥–∏ –∏ –æ–¥—ã—à–∫—É",
                "expected": [
                    ("–ü–∞—Ü–∏–µ–Ω—Ç", "O"),
                    ("–∂–∞–ª—É–µ—Ç—Å—è", "O"),
                    ("–Ω–∞", "O"),
                    ("–±–æ–ª—å", "B-SYMPTOM"),
                    ("–≤", "O"),
                    ("–≥—Ä—É–¥–∏", "B-ANATOMY"),
                    ("–∏", "O"),
                    ("–æ–¥—ã—à–∫—É", "B-SYMPTOM")
                ]
            },
            {
                "text": "–î–∏–∞–≥–Ω–æ–∑: –∏–Ω—Ñ–∞—Ä–∫—Ç –º–∏–æ–∫–∞—Ä–¥–∞",
                "expected": [
                    ("–î–∏–∞–≥–Ω–æ–∑", "O"),
                    (":", "O"),
                    ("–∏–Ω—Ñ–∞—Ä–∫—Ç", "B-DISEASE"),
                    ("–º–∏–æ–∫–∞—Ä–¥–∞", "I-DISEASE")
                ]
            },
            {
                "text": "–ù–∞–∑–Ω–∞—á–∏–ª–∏ –∞—Å–ø–∏—Ä–∏–Ω –æ—Ç –≥–æ–ª–æ–≤–Ω–æ–π –±–æ–ª–∏",
                "expected": [
                    ("–ù–∞–∑–Ω–∞—á–∏–ª–∏", "O"),
                    ("–∞—Å–ø–∏—Ä–∏–Ω", "B-DRUG"),
                    ("–æ—Ç", "O"),
                    ("–≥–æ–ª–æ–≤–Ω–æ–π", "B-SYMPTOM"),
                    ("–±–æ–ª–∏", "I-SYMPTOM")
                ]
            },
            {
                "text": "–°–¥–µ–ª–∞–ª–∏ —Ä–µ–Ω—Ç–≥–µ–Ω –ª—ë–≥–∫–∏—Ö",
                "expected": [
                    ("–°–¥–µ–ª–∞–ª–∏", "O"),
                    ("—Ä–µ–Ω—Ç–≥–µ–Ω", "B-PROCEDURE"),
                    ("–ª—ë–≥–∫–∏—Ö", "B-ANATOMY")
                ]
            },
            {
                "text": "–û–±–Ω–∞—Ä—É–∂–µ–Ω–∞ –æ–ø—É—Ö–æ–ª—å –≤ –ø–µ—á–µ–Ω–∏ —Ä–∞–∑–º–µ—Ä–æ–º 3 —Å–º",
                "expected": [
                    ("–û–±–Ω–∞—Ä—É–∂–µ–Ω–∞", "O"),
                    ("–æ–ø—É—Ö–æ–ª—å", "B-FINDING"),
                    ("–≤", "O"),
                    ("–ø–µ—á–µ–Ω–∏", "B-ANATOMY"),
                    ("—Ä–∞–∑–º–µ—Ä–æ–º", "O"),
                    ("3", "O"),
                    ("—Å–º", "O")
                ]
            }
        ]
        return test_examples
    
    def evaluate_on_custom_examples(self, model, test_examples):
        """–û—Ü–µ–Ω–∫–∞ –Ω–∞ –∫–∞—Å—Ç–æ–º–Ω—ã—Ö –ø—Ä–∏–º–µ—Ä–∞—Ö"""
        print("üß™ –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –Ω–∞ –∫–∞—Å—Ç–æ–º–Ω—ã—Ö –ø—Ä–∏–º–µ—Ä–∞—Ö...")
        
        all_true_labels = []
        all_predictions = []
        
        for example in test_examples:
            text = example["text"]
            expected = example["expected"]
            
            # –ü–æ–ª—É—á–∞–µ–º –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è
            predictions = self.predict_single_text(model, text)
            
            print(f"\nüìù –¢–µ–∫—Å—Ç: {text}")
            print("üëÅÔ∏è –û–∂–∏–¥–∞–µ–º–æ–µ vs ü§ñ –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–Ω–æ–µ:")
            
            # –°—Ä–∞–≤–Ω–∏–≤–∞–µ–º —Ç–æ–∫–µ–Ω –∫ —Ç–æ–∫–µ–Ω—É
            for i, (token, pred_label) in enumerate(predictions):
                if i < len(expected):
                    expected_token, expected_label = expected[i]
                    match = "‚úÖ" if pred_label == expected_label else "‚ùå"
                    print(f"  {token} -> {pred_label} (–æ–∂–∏–¥–∞–ª–æ—Å—å: {expected_label}) {match}")
                else:
                    print(f"  {token} -> {pred_label} (–ª–∏—à–Ω–∏–π —Ç–æ–∫–µ–Ω)")
            
            # –°–æ–±–∏—Ä–∞–µ–º –º–µ—Ç–∫–∏ –¥–ª—è –æ–±—â–∏—Ö –º–µ—Ç—Ä–∏–∫
            pred_labels = [str(label) if isinstance(label, int) else label for _, label in predictions]
            true_labels = [str(label) if isinstance(label, int) else label for _, label in expected]
            
            # –í—ã—Ä–∞–≤–Ω–∏–≤–∞–µ–º –¥–ª–∏–Ω—ã
            min_len = min(len(pred_labels), len(true_labels))
            if min_len > 0:
                all_predictions.append(pred_labels[:min_len])
                all_true_labels.append(true_labels[:min_len])
        
        # –í—ã—á–∏—Å–ª—è–µ–º –æ–±—â–∏–µ –º–µ—Ç—Ä–∏–∫–∏
        if all_true_labels and all_predictions:
            metrics = {
                "accuracy": accuracy_score(all_true_labels, all_predictions),
                "precision": precision_score(all_true_labels, all_predictions),
                "recall": recall_score(all_true_labels, all_predictions),
                "f1": f1_score(all_true_labels, all_predictions),
            }
            return metrics
        else:
            return {}
    
    def compare_models(self, test_size=50):
        """–ü–æ–ª–Ω–æ–µ —Å—Ä–∞–≤–Ω–µ–Ω–∏–µ –±–∞–∑–æ–≤–æ–π –º–æ–¥–µ–ª–∏ –∏ LoRA –º–æ–¥–µ–ª–∏"""
        print("üîÑ –ù–∞—á–∏–Ω–∞–µ–º –ø–æ–ª–Ω–æ–µ —Å—Ä–∞–≤–Ω–µ–Ω–∏–µ –º–æ–¥–µ–ª–µ–π...\n")
        
        # –ó–∞–≥—Ä—É–∂–∞–µ–º –º–æ–¥–µ–ª–∏
        base_model = self.load_base_model()
        lora_model = self.load_lora_model()
        
        if lora_model is None:
            print("‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å LoRA –º–æ–¥–µ–ª—å")
            return
        
        # –°–æ–∑–¥–∞—ë–º —Ç–µ—Å—Ç–æ–≤—ã–µ –ø—Ä–∏–º–µ—Ä—ã
        test_examples = self.create_test_examples()
        
        print("=" * 60)
        print("üß™ –¢–ï–°–¢ 1: –ö–∞—Å—Ç–æ–º–Ω—ã–µ –ø—Ä–∏–º–µ—Ä—ã")
        print("=" * 60)
        
        print("\nü§ñ –ë–∞–∑–æ–≤–∞—è –º–æ–¥–µ–ª—å:")
        base_custom_metrics = self.evaluate_on_custom_examples(base_model, test_examples)
        
        print("\nüöÄ LoRA –º–æ–¥–µ–ª—å:")
        lora_custom_metrics = self.evaluate_on_custom_examples(lora_model, test_examples)
        
        print("\nüìä –°—Ä–∞–≤–Ω–µ–Ω–∏–µ –º–µ—Ç—Ä–∏–∫ –Ω–∞ –∫–∞—Å—Ç–æ–º–Ω—ã—Ö –ø—Ä–∏–º–µ—Ä–∞—Ö:")
        print(f"{'–ú–µ—Ç—Ä–∏–∫–∞':<12} {'–ë–∞–∑–æ–≤–∞—è':<10} {'LoRA':<10} {'–£–ª—É—á—à–µ–Ω–∏–µ':<12}")
        print("-" * 50)
        for metric in ["accuracy", "precision", "recall", "f1"]:
            base_val = base_custom_metrics.get(metric, 0)
            lora_val = lora_custom_metrics.get(metric, 0)
            improvement = lora_val - base_val
            improvement_str = f"+{improvement:.3f}" if improvement > 0 else f"{improvement:.3f}"
            print(f"{metric:<12} {base_val:<10.3f} {lora_val:<10.3f} {improvement_str:<12}")
        
        print("\n" + "=" * 60)
        print("üß™ –¢–ï–°–¢ 2: –†–µ–∞–ª—å–Ω—ã–π –¥–∞—Ç–∞—Å–µ—Ç")
        print("=" * 60)
        
        print("\nü§ñ –û—Ü–µ–Ω–∫–∞ –±–∞–∑–æ–≤–æ–π –º–æ–¥–µ–ª–∏ –Ω–∞ –¥–∞—Ç–∞—Å–µ—Ç–µ...")
        base_metrics, _, _ = self.evaluate_on_dataset(base_model, test_size)
        
        print("\nüöÄ –û—Ü–µ–Ω–∫–∞ LoRA –º–æ–¥–µ–ª–∏ –Ω–∞ –¥–∞—Ç–∞—Å–µ—Ç–µ...")
        lora_metrics, true_labels, lora_predictions = self.evaluate_on_dataset(lora_model, test_size)
        
        if base_metrics and lora_metrics:
            print("\nüìä –°—Ä–∞–≤–Ω–µ–Ω–∏–µ –º–µ—Ç—Ä–∏–∫ –Ω–∞ —Ä–µ–∞–ª—å–Ω–æ–º –¥–∞—Ç–∞—Å–µ—Ç–µ:")
            print(f"{'–ú–µ—Ç—Ä–∏–∫–∞':<12} {'–ë–∞–∑–æ–≤–∞—è':<10} {'LoRA':<10} {'–£–ª—É—á—à–µ–Ω–∏–µ':<12}")
            print("-" * 50)
            for metric in ["accuracy", "precision", "recall", "f1"]:
                base_val = base_metrics[metric]
                lora_val = lora_metrics[metric]
                improvement = lora_val - base_val
                improvement_str = f"+{improvement:.3f}" if improvement > 0 else f"{improvement:.3f}"
                print(f"{metric:<12} {base_val:<10.3f} {lora_val:<10.3f} {improvement_str:<12}")
        
        # –î–µ—Ç–∞–ª—å–Ω—ã–π –æ—Ç—á—ë—Ç
        if true_labels and lora_predictions:
            print("\nüìã –î–µ—Ç–∞–ª—å–Ω—ã–π –æ—Ç—á—ë—Ç –ø–æ –∫–ª–∞—Å—Å–∞–º:")
            report = classification_report(true_labels, lora_predictions)
            print(report)
        
        print("\nüéâ –°—Ä–∞–≤–Ω–µ–Ω–∏–µ –∑–∞–≤–µ—Ä—à–µ–Ω–æ!")
        
        return {
            "custom_metrics": {"base": base_custom_metrics, "lora": lora_custom_metrics},
            "dataset_metrics": {"base": base_metrics, "lora": lora_metrics}
        }

def main():
    """–ì–ª–∞–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –¥–ª—è –∑–∞–ø—É—Å–∫–∞ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è"""
    print("üöÄ –ó–∞–ø—É—Å–∫ —Å–∏—Å—Ç–µ–º—ã —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è LoRA –º–æ–¥–µ–ª–∏")
    print("=" * 60)
    
    # –°–æ–∑–¥–∞—ë–º —Ç–µ—Å—Ç–µ—Ä
    tester = ModelTester()
    
    # –ó–∞–ø—É—Å–∫–∞–µ–º –ø–æ–ª–Ω–æ–µ —Å—Ä–∞–≤–Ω–µ–Ω–∏–µ
    results = tester.compare_models(test_size=100)
    
    print("\n‚úÖ –í—Å–µ —Ç–µ—Å—Ç—ã –∑–∞–≤–µ—Ä—à–µ–Ω—ã!")

if __name__ == "__main__":
    main()
